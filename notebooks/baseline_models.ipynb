{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LassoCV,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "# Read-in the full data set\n",
    "data = pd.read_csv('Final_Dataframe.csv')\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "## train, test split##\n",
    "\n",
    "# A train/test split is constructed where 90% of the subsample is \n",
    "# the train data set and 10% the test data set.\n",
    "\n",
    "# Set train and test sizes\n",
    "train_size = 0.9\n",
    "test_size = 1-train_size\n",
    "\n",
    "# Function to return random train and test sets\n",
    "def data_splitter(df, train, validate=False, seed=9001):\n",
    "    \n",
    "    if validate:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        validate_end = int(validate * m) + train_end\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        validate = df.ix[perm[train_end:validate_end]]\n",
    "        test = df.ix[perm[validate_end:]]\n",
    "        return train, validate, test\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        test = df.ix[perm[train_end:]]\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (1278, 949)\n",
      "Test Size: (142, 949)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test dataframes from subsample\n",
    "train_df, test_df = data_splitter(data, train_size)\n",
    "\n",
    "# Return shapes of train and test dataframes\n",
    "print(\"Train Size: {}\".format(train_df.shape))\n",
    "print(\"Test Size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "# Median imputation of missing values\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=1)\n",
    "train_df = pd.DataFrame(imp.fit_transform(train_df), columns=data.columns)\n",
    "test_df = pd.DataFrame(imp.transform(test_df), columns=data.columns)\n",
    "\n",
    "train_df = train_df[train_df['Followers'] != 0]\n",
    "test_df = test_df[test_df['Followers'] != 0]\n",
    "\n",
    "# Final step: create y_train/x_train and y_test/x_test dataframes\n",
    "\n",
    "# Initialize the training data\n",
    "y_train = np.log(train_df['Followers'])\n",
    "x_train = train_df.drop('Followers', axis=1)\n",
    "\n",
    "# Initialize the testing data\n",
    "y_test = np.log(test_df['Followers'])\n",
    "x_test = test_df.drop('Followers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, our goal is to examine the baseline performance of simple models on the test set. We would use these baseline test set r2 score as a reference for building more complex models. The models included in this section are mostly multilinear regression models with different subset of predictors and possible polynomial/interaction terms. PCA and LASSO/RRIDGE are explored here as well. \n",
    "\n",
    "We choose to start with linear regression because of high interpretability and low computational compexity. We can easily interpret the result of regression coeffcients in such fashion: holding all other predictors constant, one unit incrase in the specified predictor leads to a change in units indicated by the coefficent in the response variables. Linear regression also has a closed form solution for coefficients which reduces computational complexity.\n",
    "\n",
    "Linear regression has the following assumptions: \n",
    "* There is a linear relationship between response varibles and predictors\n",
    "* Residuals are independent\n",
    "* Residuals are normally distributed\n",
    "* Residuals has constant variance \n",
    "\n",
    "To evaluate baseline models, I use the metric r2. R2 is the proportion of overall variability of Y explained by the model. R2 has a caveat in the sense that it will always go up for the training set as we include more predictors. R2 will tend towards overfitting if we conduct model selection through R2. However, model selection in not the main goal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(a) Multilinear regression model with all predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we fit a multilinear regression with all predictors that we have. This model would not neccessarily perform well since it has 949 predictors. However, we want to conduct a sanity check through this model to make sure that prediction power is reasonable.\n",
    "\n",
    "The test r2 score for multilinear regression model with all predictors is -2.779. Our prediction does not perform well compared to the mean of response varaibles. This is evidence suggesting that we are overfitting our model with too many predictors. Therefore, going forward, we would like to fit a regression model with a subset of predictors in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For multilinear regression with all terms,R2 score for training set: 0.7655523815712503\n",
      "For multilinear regression with all terms,R2 score for test set: -2.779712758436489\n"
     ]
    }
   ],
   "source": [
    "X=sm.add_constant(x_train)\n",
    "X_test=sm.add_constant(x_test)\n",
    "model=sm.OLS(y_train,X)\n",
    "results=model.fit()\n",
    "r2_test_a=r2_score(y_test,results.predict(X_test))\n",
    "print(\"For multilinear regression with all terms,R2 score for training set: {}\".format(r2_score(y_train,results.predict(X))))\n",
    "print(\"For multilinear regression with all terms,R2 score for test set: {}\".format(r2_test_a))\n",
    "#results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part(b) Multilinear regression model with top artists predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our prelimnary EDA anlaysis, we believed that top artists would be a good predictor for the success of a playlist. Therefore, here we fit two models in part (b) and (c) that only include predictors including artists. In part (b), we use the dummy variables of top 30 artists as predictors. As a note, for part (b), top artist are those who appear most often in playlists with 350,000+ followers. With more than 350,000 followers, a playlist will beat 80% of playlists in terms of followers. \n",
    "\n",
    "Our regression generates a test r2 score of 0.017, which is a lot better than -2.8 in part (a). Therefore, there is good reason to consider these predictors in future model building.\n",
    "\n",
    "From the regression summary table, we get the list of significant top 30 artist predictors:\n",
    "* 'Galantis', 'Post Malone', 'Yo Gotti', 'Ellie Goulding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tio 30 artists,R2 score for test set: 0.017373740567596885\n"
     ]
    }
   ],
   "source": [
    "top_30_artist_col=['Lil Wayne', 'Van Morrison', 'Galantis',\n",
    "       'Wiz Khalifa', 'Rihanna', 'Post Malone', 'Axwell /\\ Ingrosso',\n",
    "       'Young Thug', 'JAY Z', 'A$AP Rocky', 'Yo Gotti', 'Chance The Rapper',\n",
    "       'Led Zeppelin', 'Otis Redding', '21 Savage', 'Deorro', 'Elton John',\n",
    "       'SZA', 'Ty Dolla $ign', 'Ryan Adams', 'Birdy', 'Miguel', 'Niall Horan',\n",
    "       'Ellie Goulding', 'Commodores', 'Radiohead', 'SYML', 'First Aid Kit',\n",
    "       'Lord Huron']\n",
    "\n",
    "x_train_art=x_train[top_30_artist_col]\n",
    "x_test_art=x_test[top_30_artist_col]\n",
    "\n",
    "X1=sm.add_constant(x_train_art)\n",
    "X2=sm.add_constant(x_test_art)\n",
    "model2=sm.OLS(y_train,X1)\n",
    "results2=model2.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With top 30 artists,R2 score for training set: 0.06172694503105225\n",
      "With top 30 artists,R2 score for test set: 0.017373740567596885\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Followers</td>    <th>  R-squared:         </th> <td>   0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 06 Dec 2017</td> <th>  Prob (F-statistic):</th> <td>1.60e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:37:49</td>     <th>  Log-Likelihood:    </th> <td> -3134.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1257</td>      <th>  AIC:               </th> <td>   6329.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1227</td>      <th>  BIC:               </th> <td>   6483.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    29</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>              <td>    9.5080</td> <td>    0.091</td> <td>  104.136</td> <td> 0.000</td> <td>    9.329</td> <td>    9.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lil Wayne</th>          <td>    0.8115</td> <td>    0.828</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.813</td> <td>    2.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Van Morrison</th>       <td>    0.0322</td> <td>    0.833</td> <td>    0.039</td> <td> 0.969</td> <td>   -1.603</td> <td>    1.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Galantis</th>           <td>    3.3362</td> <td>    1.093</td> <td>    3.051</td> <td> 0.002</td> <td>    1.191</td> <td>    5.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Wiz Khalifa</th>        <td>    0.3097</td> <td>    0.889</td> <td>    0.348</td> <td> 0.728</td> <td>   -1.435</td> <td>    2.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Rihanna</th>            <td>    0.5662</td> <td>    0.777</td> <td>    0.728</td> <td> 0.467</td> <td>   -0.959</td> <td>    2.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Post Malone</th>        <td>    2.4444</td> <td>    1.041</td> <td>    2.349</td> <td> 0.019</td> <td>    0.402</td> <td>    4.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Axwell /\\ Ingrosso</th> <td>    2.0688</td> <td>    1.121</td> <td>    1.846</td> <td> 0.065</td> <td>   -0.130</td> <td>    4.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Young Thug</th>         <td>   -1.0316</td> <td>    0.937</td> <td>   -1.101</td> <td> 0.271</td> <td>   -2.870</td> <td>    0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JAY Z</th>              <td>    0.6612</td> <td>    0.756</td> <td>    0.875</td> <td> 0.382</td> <td>   -0.822</td> <td>    2.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>A$AP Rocky</th>         <td>    0.7276</td> <td>    0.969</td> <td>    0.751</td> <td> 0.453</td> <td>   -1.174</td> <td>    2.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Yo Gotti</th>           <td>    3.6842</td> <td>    1.057</td> <td>    3.485</td> <td> 0.001</td> <td>    1.610</td> <td>    5.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Chance The Rapper</th>  <td>    1.9356</td> <td>    1.074</td> <td>    1.801</td> <td> 0.072</td> <td>   -0.172</td> <td>    4.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Led Zeppelin</th>       <td>    1.7855</td> <td>    0.925</td> <td>    1.931</td> <td> 0.054</td> <td>   -0.029</td> <td>    3.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Otis Redding</th>       <td>    0.3608</td> <td>    0.989</td> <td>    0.365</td> <td> 0.715</td> <td>   -1.580</td> <td>    2.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21 Savage</th>          <td>    2.1001</td> <td>    1.173</td> <td>    1.790</td> <td> 0.074</td> <td>   -0.202</td> <td>    4.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Deorro</th>             <td>    1.9751</td> <td>    1.074</td> <td>    1.840</td> <td> 0.066</td> <td>   -0.131</td> <td>    4.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Elton John</th>         <td>    1.2259</td> <td>    0.875</td> <td>    1.401</td> <td> 0.161</td> <td>   -0.491</td> <td>    2.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SZA</th>                <td>    0.9203</td> <td>    0.914</td> <td>    1.007</td> <td> 0.314</td> <td>   -0.873</td> <td>    2.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Ty Dolla $ign</th>      <td>    0.7080</td> <td>    0.955</td> <td>    0.741</td> <td> 0.459</td> <td>   -1.166</td> <td>    2.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Ryan Adams</th>         <td>    0.8033</td> <td>    0.744</td> <td>    1.080</td> <td> 0.280</td> <td>   -0.656</td> <td>    2.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Birdy</th>              <td>    0.5316</td> <td>    1.076</td> <td>    0.494</td> <td> 0.621</td> <td>   -1.579</td> <td>    2.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Miguel</th>             <td>    1.0180</td> <td>    1.024</td> <td>    0.994</td> <td> 0.320</td> <td>   -0.991</td> <td>    3.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Niall Horan</th>        <td>    1.3225</td> <td>    0.952</td> <td>    1.390</td> <td> 0.165</td> <td>   -0.545</td> <td>    3.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Ellie Goulding</th>     <td>    1.7334</td> <td>    0.803</td> <td>    2.159</td> <td> 0.031</td> <td>    0.158</td> <td>    3.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Commodores</th>         <td>    0.9663</td> <td>    1.004</td> <td>    0.962</td> <td> 0.336</td> <td>   -1.004</td> <td>    2.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radiohead</th>          <td>    0.2564</td> <td>    0.882</td> <td>    0.291</td> <td> 0.771</td> <td>   -1.474</td> <td>    1.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SYML</th>               <td>    1.8656</td> <td>    1.176</td> <td>    1.586</td> <td> 0.113</td> <td>   -0.442</td> <td>    4.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>First Aid Kit</th>      <td>    0.9468</td> <td>    0.958</td> <td>    0.988</td> <td> 0.323</td> <td>   -0.933</td> <td>    2.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lord Huron</th>         <td>    0.5109</td> <td>    0.861</td> <td>    0.593</td> <td> 0.553</td> <td>   -1.179</td> <td>    2.201</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>79.124</td> <th>  Durbin-Watson:     </th> <td>   1.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  88.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.630</td> <th>  Prob(JB):          </th> <td>5.34e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.672</td> <th>  Cond. No.          </th> <td>    15.2</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Followers   R-squared:                       0.062\n",
       "Model:                            OLS   Adj. R-squared:                  0.040\n",
       "Method:                 Least Squares   F-statistic:                     2.784\n",
       "Date:                Wed, 06 Dec 2017   Prob (F-statistic):           1.60e-06\n",
       "Time:                        13:37:49   Log-Likelihood:                -3134.4\n",
       "No. Observations:                1257   AIC:                             6329.\n",
       "Df Residuals:                    1227   BIC:                             6483.\n",
       "Df Model:                          29                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "const                  9.5080      0.091    104.136      0.000       9.329       9.687\n",
       "Lil Wayne              0.8115      0.828      0.980      0.327      -0.813       2.436\n",
       "Van Morrison           0.0322      0.833      0.039      0.969      -1.603       1.667\n",
       "Galantis               3.3362      1.093      3.051      0.002       1.191       5.481\n",
       "Wiz Khalifa            0.3097      0.889      0.348      0.728      -1.435       2.054\n",
       "Rihanna                0.5662      0.777      0.728      0.467      -0.959       2.091\n",
       "Post Malone            2.4444      1.041      2.349      0.019       0.402       4.486\n",
       "Axwell /\\ Ingrosso     2.0688      1.121      1.846      0.065      -0.130       4.267\n",
       "Young Thug            -1.0316      0.937     -1.101      0.271      -2.870       0.806\n",
       "JAY Z                  0.6612      0.756      0.875      0.382      -0.822       2.144\n",
       "A$AP Rocky             0.7276      0.969      0.751      0.453      -1.174       2.629\n",
       "Yo Gotti               3.6842      1.057      3.485      0.001       1.610       5.758\n",
       "Chance The Rapper      1.9356      1.074      1.801      0.072      -0.172       4.043\n",
       "Led Zeppelin           1.7855      0.925      1.931      0.054      -0.029       3.600\n",
       "Otis Redding           0.3608      0.989      0.365      0.715      -1.580       2.301\n",
       "21 Savage              2.1001      1.173      1.790      0.074      -0.202       4.402\n",
       "Deorro                 1.9751      1.074      1.840      0.066      -0.131       4.081\n",
       "Elton John             1.2259      0.875      1.401      0.161      -0.491       2.943\n",
       "SZA                    0.9203      0.914      1.007      0.314      -0.873       2.713\n",
       "Ty Dolla $ign          0.7080      0.955      0.741      0.459      -1.166       2.582\n",
       "Ryan Adams             0.8033      0.744      1.080      0.280      -0.656       2.262\n",
       "Birdy                  0.5316      1.076      0.494      0.621      -1.579       2.642\n",
       "Miguel                 1.0180      1.024      0.994      0.320      -0.991       3.027\n",
       "Niall Horan            1.3225      0.952      1.390      0.165      -0.545       3.190\n",
       "Ellie Goulding         1.7334      0.803      2.159      0.031       0.158       3.309\n",
       "Commodores             0.9663      1.004      0.962      0.336      -1.004       2.937\n",
       "Radiohead              0.2564      0.882      0.291      0.771      -1.474       1.987\n",
       "SYML                   1.8656      1.176      1.586      0.113      -0.442       4.173\n",
       "First Aid Kit          0.9468      0.958      0.988      0.323      -0.933       2.827\n",
       "Lord Huron             0.5109      0.861      0.593      0.553      -1.179       2.201\n",
       "==============================================================================\n",
       "Omnibus:                       79.124   Durbin-Watson:                   1.955\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               88.754\n",
       "Skew:                          -0.630   Prob(JB):                     5.34e-20\n",
       "Kurtosis:                       2.672   Cond. No.                         15.2\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With top 30 artists,R2 score for training set: {}\".format(r2_score(y_train,results2.predict(X1))))\n",
    "print(\"With top 30 artists,R2 score for test set: {}\".format(r2_score(y_test,results2.predict(X2))))\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Galantis', 'Post Malone', 'Yo Gotti', 'Ellie Goulding'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#significant artistis\n",
    "results2.pvalues[results2.pvalues < 0.05].index[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part(c) Multilinear regression model with top artists counts predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to evaluate artist predictors in part (c). Here, top artists are defined differently from part (b).We first sum up the total number of followers for playlists that include an artist. Then, we rank the artists basing on the aggregated playlist followers. For part (c), the predictors are the number of top 10/10-20/20-30/30-40/40-50 artists that a playlist has.\n",
    "\n",
    "From regression result, we see that r2 training result is 0.023 and test result is -0.03. The significant predictors include: \"top_30_40\", \"top_40_50\". It seems like part(c) r2 test score are lower than that in part(b), indicating that part(b) artist predictors has more power in predicting playlist followers thatn part(c) predictors. We should put more emphasis on part(b) predictors when constructing our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hide": true
   },
   "outputs": [],
   "source": [
    "x_train_art_count=x_train[top_artist_count_columns]\n",
    "x_test_art_count=x_test[top_artist_count_columns]\n",
    "\n",
    "X3=sm.add_constant(x_train_art_count)\n",
    "X4=sm.add_constant(x_test_art_count)\n",
    "model3=sm.OLS(y_train,X3)\n",
    "results3=model3.fit()\n",
    "#print(\"With top artists count predictors,R2 score for test set: {}\".format(r2_score(y_test,results3.predict(X4))))\n",
    "#results3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With top artists count predictors,R2 score for training set: 0.012645473737053159\n",
      "With top artists count predictors,R2 score for test set: -0.031235636069632644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Followers</td>    <th>  R-squared:         </th> <td>   0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 06 Dec 2017</td> <th>  Prob (F-statistic):</th>  <td>0.00702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:46:04</td>     <th>  Log-Likelihood:    </th> <td> -3166.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1257</td>      <th>  AIC:               </th> <td>   6345.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1251</td>      <th>  BIC:               </th> <td>   6376.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    9.6438</td> <td>    0.097</td> <td>   99.900</td> <td> 0.000</td> <td>    9.454</td> <td>    9.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_0_10</th>  <td>    0.1075</td> <td>    0.274</td> <td>    0.392</td> <td> 0.695</td> <td>   -0.431</td> <td>    0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_10_20</th> <td>    0.2819</td> <td>    0.358</td> <td>    0.788</td> <td> 0.431</td> <td>   -0.420</td> <td>    0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_20_30</th> <td>   -0.1965</td> <td>    0.280</td> <td>   -0.702</td> <td> 0.483</td> <td>   -0.745</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_30_40</th> <td>    0.7624</td> <td>    0.273</td> <td>    2.789</td> <td> 0.005</td> <td>    0.226</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_40_50</th> <td>    0.7486</td> <td>    0.321</td> <td>    2.330</td> <td> 0.020</td> <td>    0.118</td> <td>    1.379</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>79.718</td> <th>  Durbin-Watson:     </th> <td>   1.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  83.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.595</td> <th>  Prob(JB):          </th> <td>6.73e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.574</td> <th>  Cond. No.          </th> <td>    4.32</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Followers   R-squared:                       0.013\n",
       "Model:                            OLS   Adj. R-squared:                  0.009\n",
       "Method:                 Least Squares   F-statistic:                     3.204\n",
       "Date:                Wed, 06 Dec 2017   Prob (F-statistic):            0.00702\n",
       "Time:                        13:46:04   Log-Likelihood:                -3166.5\n",
       "No. Observations:                1257   AIC:                             6345.\n",
       "Df Residuals:                    1251   BIC:                             6376.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          9.6438      0.097     99.900      0.000       9.454       9.833\n",
       "top_0_10       0.1075      0.274      0.392      0.695      -0.431       0.646\n",
       "top_10_20      0.2819      0.358      0.788      0.431      -0.420       0.984\n",
       "top_20_30     -0.1965      0.280     -0.702      0.483      -0.745       0.352\n",
       "top_30_40      0.7624      0.273      2.789      0.005       0.226       1.299\n",
       "top_40_50      0.7486      0.321      2.330      0.020       0.118       1.379\n",
       "==============================================================================\n",
       "Omnibus:                       79.718   Durbin-Watson:                   1.945\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               83.686\n",
       "Skew:                          -0.595   Prob(JB):                     6.73e-19\n",
       "Kurtosis:                       2.574   Cond. No.                         4.32\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With top artists count predictors,R2 score for training set: {}\".format(r2_score(y_train,results3.predict(X3))))\n",
    "print(\"With top artists count predictors,R2 score for test set: {}\".format(r2_score(y_test,results3.predict(X4))))\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part(d) Multilinear regression model with genre predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our prelimnary EDA anlaysis, we also believed that genres would be a good predictor for playlist followers. Therefore, we fit a regression model with only genre predictors. Hereï¼Œeach predictors is a categorical variable indicating whether the playlist belongs to a specified genre.\n",
    "\n",
    "When we use all of genre predictors, we see that training r2 score is 0.59 and test r2 score is -3.785. This again is the result of overfitting since we have 865 genre columns. Therefore, we fit another regression model with only significant genre predictors from the full genre regression model. This time, we have a training r2 score of 0.009 and test r2 score of 0.007. The number of significant genre predictors is 54.\n",
    "\n",
    "Therefore, a subset of genre predictors could still be important and should be considered for building future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hide": true
   },
   "outputs": [],
   "source": [
    "x_train_genre=x_train[genre_columns]\n",
    "x_test_genre=x_test[genre_columns]\n",
    "\n",
    "X5=sm.add_constant(x_train_genre)\n",
    "X6=sm.add_constant(x_test_genre)\n",
    "model4=sm.OLS(y_train,X5)\n",
    "results4=model4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all genre predictors,R2 score for training set: 0.5974648743813241\n",
      "With all genre predictors,R2 score for test set: -3.7847086814421367\n",
      "Number of genre predictors:865\n",
      "Significant genre predictors in regression:\n",
      "const                              1.678665e-87\n",
      " 'alternative metal'               4.049398e-02\n",
      " 'brooklyn indie'                  4.809165e-02\n",
      " 'christian punk'                  1.321873e-02\n",
      " 'classic rock'                    6.111137e-03\n",
      " 'country christmas'               2.945994e-02\n",
      " 'dirty south rap'                 4.232302e-02\n",
      " 'dixieland'                       1.657556e-02\n",
      " 'ectofolk'                        3.147615e-02\n",
      " 'filter house'                    6.523992e-03\n",
      " 'garage punk'                     4.496653e-02\n",
      " 'garage rock'                     1.192386e-02\n",
      " 'hauntology'                      3.862767e-02\n",
      " 'indie garage rock'               1.106413e-02\n",
      " 'indie psych-pop'                 4.345163e-02\n",
      " 'industrial rock'                 3.083634e-02\n",
      " 'industrial'                      4.519249e-02\n",
      " 'jangle pop'                      2.091888e-02\n",
      " 'kraut rock'                      1.918933e-02\n",
      " 'memphis hip hop'                 4.410499e-02\n",
      " 'metal'                           2.472603e-02\n",
      " 'modern blues'                    4.417157e-02\n",
      " 'new orleans jazz'                2.401043e-02\n",
      " 'outsider'                        1.112002e-02\n",
      " 'pop punk'                        2.095434e-02\n",
      " 'post-teen pop'                   4.203491e-02\n",
      " 'power metal'                     3.328351e-02\n",
      " 'power pop'                       2.371500e-02\n",
      " 'progressive electro house'       2.438308e-02\n",
      " 'progressive house'               3.076225e-02\n",
      " 'progressive uplifting trance'    2.805253e-02\n",
      " 'pub rock'                        3.587669e-02\n",
      " 'rap metal'                       1.668566e-02\n",
      " 'retro electro'                   6.837326e-03\n",
      " 'retro metal'                     4.648427e-02\n",
      " 'shibuya-kei'                     2.839880e-02\n",
      " 'soft rock'                       6.490938e-03\n",
      " 'soul christmas'                  4.282753e-02\n",
      " 'stomp and holler'                3.701798e-03\n",
      " 'traditional folk'                2.421117e-02\n",
      "\"children's christmas\"             1.072646e-02\n",
      "'abstractro'                       4.026550e-02\n",
      "'afrobeat'                         1.706527e-02\n",
      "'alternative americana'            1.569490e-02\n",
      "'alternative rock'                 2.427948e-02\n",
      "'art rock'                         4.098453e-02\n",
      "'blues-rock'                       5.686620e-03\n",
      "'country gospel'                   1.407087e-02\n",
      "'dance-punk'                       1.954791e-03\n",
      "'deep new americana'               1.595271e-02\n",
      "'escape room'                      2.422625e-02\n",
      "'g funk'                           5.312503e-03\n",
      "'metalcore'                        1.137579e-02\n",
      "'neo-synthpop'                     4.452120e-02\n",
      "'psychedelic doom'                 4.648427e-02\n",
      "'speed garage'                     2.949958e-03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"With all genre predictors,R2 score for training set: {}\".format(r2_score(y_train,results4.predict(X5))))\n",
    "print(\"With all genre predictors,R2 score for test set: {}\".format(r2_score(y_test,results4.predict(X6))))\n",
    "print(\"Number of genre predictors:{}\".format(len(genre_columns)))\n",
    "print(\"Significant genre predictors in regression:\\n{}\".format(results4.pvalues[results4.pvalues < 0.05]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "sig_genre=results4.pvalues[results4.pvalues < 0.05].index[1:55]\n",
    "x_train_genre=x_train[sig_genre]\n",
    "x_test_genre=x_test[sig_genre]\n",
    "\n",
    "X9=sm.add_constant(x_train_genre)\n",
    "X10=sm.add_constant(x_test_genre)\n",
    "model9=sm.OLS(y_train,X9)\n",
    "results9=model9.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With significant genre predictor,R2 score for training set: 0.09225710866655823\n",
      "With significant genre predictor,R2 score for test set: 0.007097541941422314\n",
      "Number of genre predictors:54\n"
     ]
    }
   ],
   "source": [
    "print(\"With significant genre predictor,R2 score for training set: {}\".format(r2_score(y_train,results9.predict(X9))))\n",
    "print(\"With significant genre predictor,R2 score for test set: {}\".format(r2_score(y_test,results9.predict(X10))))\n",
    "print(\"Number of genre predictors:{}\".format(len(sig_genre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Multilinear regression with only siginifcant predictors from part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in part(a), our model includes all predictors and thus tend towards overfitting. In part(e), we fit a model with siginificant predictors from model in part(a) to reduce overfitting. We have a total of 49 predictors (cut down from 949 originally).\n",
    "\n",
    "We see that test r2 score goes up to 0.085, which is the best r2 score so far. This indicates that our previous model indeed suffer from overfitting. We should work on choosing a subset of original predictors as predictors for furture models to improve prediction power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant predictors from regression in part(a):\n",
      "instrumentalness_mean              0.010220\n",
      "liveness_mean                      0.001904\n",
      "loudness_std                       0.038317\n",
      "speech_mean                        0.021664\n",
      "time_std                           0.005990\n",
      "valence_mean                       0.000028\n",
      " 'bass music'                      0.006956\n",
      " 'big band'                        0.015642\n",
      " 'christian punk'                  0.042034\n",
      " 'country gospel'                  0.020285\n",
      " 'crunk'                           0.026326\n",
      " 'deep house'                      0.048536\n",
      " 'dubstep'                         0.014862\n",
      " 'ectofolk'                        0.016383\n",
      " 'electro house'                   0.004962\n",
      " 'escape room'                     0.043086\n",
      " 'experimental'                    0.021957\n",
      " 'filter house'                    0.032306\n",
      " 'garage rock'                     0.011979\n",
      " 'latin pop'                       0.031294\n",
      " 'modern blues'                    0.022673\n",
      " 'modern country rock'             0.033201\n",
      " 'new orleans jazz'                0.007798\n",
      " 'pop emo'                         0.041783\n",
      " 'pop punk'                        0.016774\n",
      " 'progressive electro house'       0.049407\n",
      " 'progressive house'               0.044417\n",
      " 'progressive uplifting trance'    0.007200\n",
      " 'traditional folk'                0.039684\n",
      " 'trip hop'                        0.044841\n",
      "'alternative rock'                 0.031086\n",
      "'austindie'                        0.007331\n",
      "'blues-rock'                       0.010399\n",
      "'canadian metal'                   0.029284\n",
      "'chillhop'                         0.002849\n",
      "'columbus ohio indie'              0.031607\n",
      "'dance-punk'                       0.032371\n",
      "'deep new americana'               0.034513\n",
      "'edm'                              0.014690\n",
      "'g funk'                           0.000820\n",
      "'indie punk'                       0.039368\n",
      "'pop'                              0.048267\n",
      "'progressive post-hardcore'        0.005071\n",
      "'speed garage'                     0.002887\n",
      "Radiohead                          0.000706\n",
      "Str_Best                           0.000583\n",
      "Str_Acoustic                       0.000383\n",
      "Str_2000s                          0.000014\n",
      "dance_liveness_std                 0.020839\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Significant predictors from regression in part(a):\\n{}\".format(results.pvalues[results.pvalues < 0.05]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hide": true
   },
   "outputs": [],
   "source": [
    "#fit a multilinear regression model with significant predictors\n",
    "sig_preds=results.pvalues[results.pvalues < 0.05].index\n",
    "x_train2 = x_train[sig_preds]\n",
    "x_test2 = x_test[sig_preds]\n",
    "\n",
    "X7=sm.add_constant(x_train2)\n",
    "X8=sm.add_constant(x_test2)\n",
    "model5=sm.OLS(y_train,X7)\n",
    "results5=model5.fit()\n",
    "r2_test_e=r2_score(y_test,results5.predict(X8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With only the significant predictors in part(a),R2 score for test set: 0.22290282653744364\n",
      "With only the significant predictors in part(a),R2 score for test set: 0.0825894720034378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Followers</td>    <th>  R-squared:         </th> <td>   0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 06 Dec 2017</td> <th>  Prob (F-statistic):</th> <td>1.25e-39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:06:25</td>     <th>  Log-Likelihood:    </th> <td> -3016.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1257</td>      <th>  AIC:               </th> <td>   6132.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1207</td>      <th>  BIC:               </th> <td>   6389.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    49</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                           <td>   15.2241</td> <td>    0.950</td> <td>   16.027</td> <td> 0.000</td> <td>   13.361</td> <td>   17.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness_mean</th>           <td>   -3.1447</td> <td>    1.089</td> <td>   -2.887</td> <td> 0.004</td> <td>   -5.282</td> <td>   -1.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness_mean</th>                   <td>  -12.1780</td> <td>    2.358</td> <td>   -5.164</td> <td> 0.000</td> <td>  -16.804</td> <td>   -7.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness_std</th>                    <td>    3.3418</td> <td>    1.299</td> <td>    2.572</td> <td> 0.010</td> <td>    0.793</td> <td>    5.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speech_mean</th>                     <td>   -2.5326</td> <td>    0.971</td> <td>   -2.608</td> <td> 0.009</td> <td>   -4.438</td> <td>   -0.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_std</th>                        <td> -1.25e-08</td> <td> 3.99e-09</td> <td>   -3.131</td> <td> 0.002</td> <td>-2.03e-08</td> <td>-4.67e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence_mean</th>                    <td>   -7.3241</td> <td>    1.413</td> <td>   -5.183</td> <td> 0.000</td> <td>  -10.096</td> <td>   -4.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'bass music'</th>                   <td>   -0.4514</td> <td>    0.405</td> <td>   -1.116</td> <td> 0.265</td> <td>   -1.245</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'big band'</th>                     <td>    0.2267</td> <td>    0.316</td> <td>    0.718</td> <td> 0.473</td> <td>   -0.393</td> <td>    0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'christian punk'</th>               <td>   -1.1516</td> <td>    0.685</td> <td>   -1.682</td> <td> 0.093</td> <td>   -2.495</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'country gospel'</th>               <td>   -0.1673</td> <td>    0.326</td> <td>   -0.514</td> <td> 0.607</td> <td>   -0.806</td> <td>    0.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'crunk'</th>                        <td>    1.4932</td> <td>    0.785</td> <td>    1.901</td> <td> 0.058</td> <td>   -0.048</td> <td>    3.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'deep house'</th>                   <td>    0.3671</td> <td>    0.299</td> <td>    1.226</td> <td> 0.220</td> <td>   -0.220</td> <td>    0.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'dubstep'</th>                      <td>    0.6460</td> <td>    0.540</td> <td>    1.196</td> <td> 0.232</td> <td>   -0.414</td> <td>    1.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'ectofolk'</th>                     <td>   -2.1576</td> <td>    1.148</td> <td>   -1.880</td> <td> 0.060</td> <td>   -4.409</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'electro house'</th>                <td>   -0.2006</td> <td>    0.233</td> <td>   -0.859</td> <td> 0.390</td> <td>   -0.659</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'escape room'</th>                  <td>    0.6030</td> <td>    0.182</td> <td>    3.308</td> <td> 0.001</td> <td>    0.245</td> <td>    0.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'experimental'</th>                 <td>    0.1237</td> <td>    0.213</td> <td>    0.580</td> <td> 0.562</td> <td>   -0.295</td> <td>    0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'filter house'</th>                 <td>    1.0122</td> <td>    0.338</td> <td>    2.993</td> <td> 0.003</td> <td>    0.349</td> <td>    1.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'garage rock'</th>                  <td>    0.2319</td> <td>    0.201</td> <td>    1.154</td> <td> 0.249</td> <td>   -0.163</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'latin pop'</th>                    <td>    0.4265</td> <td>    0.220</td> <td>    1.942</td> <td> 0.052</td> <td>   -0.004</td> <td>    0.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'modern blues'</th>                 <td>    0.5259</td> <td>    0.187</td> <td>    2.809</td> <td> 0.005</td> <td>    0.159</td> <td>    0.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'modern country rock'</th>          <td>    0.4418</td> <td>    0.190</td> <td>    2.321</td> <td> 0.020</td> <td>    0.068</td> <td>    0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'new orleans jazz'</th>             <td>   -0.4096</td> <td>    0.363</td> <td>   -1.128</td> <td> 0.260</td> <td>   -1.122</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'pop emo'</th>                      <td>   -0.1501</td> <td>    0.209</td> <td>   -0.719</td> <td> 0.472</td> <td>   -0.560</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'pop punk'</th>                     <td>    0.4633</td> <td>    0.185</td> <td>    2.504</td> <td> 0.012</td> <td>    0.100</td> <td>    0.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'progressive electro house'</th>    <td>    0.5527</td> <td>    0.332</td> <td>    1.666</td> <td> 0.096</td> <td>   -0.098</td> <td>    1.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'progressive house'</th>            <td>   -0.3463</td> <td>    0.331</td> <td>   -1.048</td> <td> 0.295</td> <td>   -0.995</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'progressive uplifting trance'</th> <td>   -0.3538</td> <td>    0.817</td> <td>   -0.433</td> <td> 0.665</td> <td>   -1.957</td> <td>    1.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'traditional folk'</th>             <td>   -0.0593</td> <td>    0.365</td> <td>   -0.163</td> <td> 0.871</td> <td>   -0.776</td> <td>    0.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> 'trip hop'</th>                     <td>    0.9485</td> <td>    0.391</td> <td>    2.429</td> <td> 0.015</td> <td>    0.182</td> <td>    1.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'alternative rock'</th>              <td>    0.1924</td> <td>    0.197</td> <td>    0.974</td> <td> 0.330</td> <td>   -0.195</td> <td>    0.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'austindie'</th>                     <td>    0.6574</td> <td>    0.505</td> <td>    1.301</td> <td> 0.194</td> <td>   -0.334</td> <td>    1.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'blues-rock'</th>                    <td>   -0.3016</td> <td>    0.243</td> <td>   -1.241</td> <td> 0.215</td> <td>   -0.778</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'canadian metal'</th>                <td>    0.7083</td> <td>    0.857</td> <td>    0.827</td> <td> 0.409</td> <td>   -0.972</td> <td>    2.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'chillhop'</th>                      <td>   -0.6586</td> <td>    0.847</td> <td>   -0.778</td> <td> 0.437</td> <td>   -2.320</td> <td>    1.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'columbus ohio indie'</th>           <td>   -0.8179</td> <td>    0.535</td> <td>   -1.528</td> <td> 0.127</td> <td>   -1.868</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'dance-punk'</th>                    <td>   -0.3826</td> <td>    0.571</td> <td>   -0.670</td> <td> 0.503</td> <td>   -1.503</td> <td>    0.738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'deep new americana'</th>            <td>    0.0373</td> <td>    0.218</td> <td>    0.171</td> <td> 0.864</td> <td>   -0.391</td> <td>    0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'edm'</th>                           <td>    0.6196</td> <td>    0.301</td> <td>    2.062</td> <td> 0.039</td> <td>    0.030</td> <td>    1.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'g funk'</th>                        <td>   -0.5112</td> <td>    0.420</td> <td>   -1.217</td> <td> 0.224</td> <td>   -1.335</td> <td>    0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'indie punk'</th>                    <td>    0.0738</td> <td>    0.430</td> <td>    0.172</td> <td> 0.864</td> <td>   -0.769</td> <td>    0.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'pop'</th>                           <td>    0.5996</td> <td>    0.276</td> <td>    2.175</td> <td> 0.030</td> <td>    0.059</td> <td>    1.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'progressive post-hardcore'</th>     <td>    0.7197</td> <td>    0.449</td> <td>    1.603</td> <td> 0.109</td> <td>   -0.161</td> <td>    1.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>'speed garage'</th>                  <td>   -0.3341</td> <td>    0.646</td> <td>   -0.517</td> <td> 0.605</td> <td>   -1.602</td> <td>    0.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radiohead</th>                       <td>   -0.8771</td> <td>    0.821</td> <td>   -1.068</td> <td> 0.286</td> <td>   -2.489</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Str_Best</th>                        <td>   -1.2647</td> <td>    0.330</td> <td>   -3.837</td> <td> 0.000</td> <td>   -1.911</td> <td>   -0.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Str_Acoustic</th>                    <td>    3.1248</td> <td>    0.617</td> <td>    5.064</td> <td> 0.000</td> <td>    1.914</td> <td>    4.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Str_2000s</th>                       <td>   -3.0511</td> <td>    0.381</td> <td>   -8.006</td> <td> 0.000</td> <td>   -3.799</td> <td>   -2.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dance_liveness_std</th>              <td>   -0.1096</td> <td>    0.030</td> <td>   -3.650</td> <td> 0.000</td> <td>   -0.169</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>31.737</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  33.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.395</td> <th>  Prob(JB):          </th> <td>4.87e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.868</td> <th>  Cond. No.          </th> <td>6.39e+08</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Followers   R-squared:                       0.223\n",
       "Model:                            OLS   Adj. R-squared:                  0.191\n",
       "Method:                 Least Squares   F-statistic:                     7.066\n",
       "Date:                Wed, 06 Dec 2017   Prob (F-statistic):           1.25e-39\n",
       "Time:                        14:06:25   Log-Likelihood:                -3016.0\n",
       "No. Observations:                1257   AIC:                             6132.\n",
       "Df Residuals:                    1207   BIC:                             6389.\n",
       "Df Model:                          49                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "const                              15.2241      0.950     16.027      0.000      13.361      17.088\n",
       "instrumentalness_mean              -3.1447      1.089     -2.887      0.004      -5.282      -1.008\n",
       "liveness_mean                     -12.1780      2.358     -5.164      0.000     -16.804      -7.552\n",
       "loudness_std                        3.3418      1.299      2.572      0.010       0.793       5.891\n",
       "speech_mean                        -2.5326      0.971     -2.608      0.009      -4.438      -0.627\n",
       "time_std                         -1.25e-08   3.99e-09     -3.131      0.002   -2.03e-08   -4.67e-09\n",
       "valence_mean                       -7.3241      1.413     -5.183      0.000     -10.096      -4.552\n",
       " 'bass music'                      -0.4514      0.405     -1.116      0.265      -1.245       0.342\n",
       " 'big band'                         0.2267      0.316      0.718      0.473      -0.393       0.847\n",
       " 'christian punk'                  -1.1516      0.685     -1.682      0.093      -2.495       0.192\n",
       " 'country gospel'                  -0.1673      0.326     -0.514      0.607      -0.806       0.471\n",
       " 'crunk'                            1.4932      0.785      1.901      0.058      -0.048       3.034\n",
       " 'deep house'                       0.3671      0.299      1.226      0.220      -0.220       0.954\n",
       " 'dubstep'                          0.6460      0.540      1.196      0.232      -0.414       1.706\n",
       " 'ectofolk'                        -2.1576      1.148     -1.880      0.060      -4.409       0.094\n",
       " 'electro house'                   -0.2006      0.233     -0.859      0.390      -0.659       0.257\n",
       " 'escape room'                      0.6030      0.182      3.308      0.001       0.245       0.961\n",
       " 'experimental'                     0.1237      0.213      0.580      0.562      -0.295       0.543\n",
       " 'filter house'                     1.0122      0.338      2.993      0.003       0.349       1.676\n",
       " 'garage rock'                      0.2319      0.201      1.154      0.249      -0.163       0.626\n",
       " 'latin pop'                        0.4265      0.220      1.942      0.052      -0.004       0.857\n",
       " 'modern blues'                     0.5259      0.187      2.809      0.005       0.159       0.893\n",
       " 'modern country rock'              0.4418      0.190      2.321      0.020       0.068       0.815\n",
       " 'new orleans jazz'                -0.4096      0.363     -1.128      0.260      -1.122       0.303\n",
       " 'pop emo'                         -0.1501      0.209     -0.719      0.472      -0.560       0.259\n",
       " 'pop punk'                         0.4633      0.185      2.504      0.012       0.100       0.826\n",
       " 'progressive electro house'        0.5527      0.332      1.666      0.096      -0.098       1.203\n",
       " 'progressive house'               -0.3463      0.331     -1.048      0.295      -0.995       0.302\n",
       " 'progressive uplifting trance'    -0.3538      0.817     -0.433      0.665      -1.957       1.250\n",
       " 'traditional folk'                -0.0593      0.365     -0.163      0.871      -0.776       0.657\n",
       " 'trip hop'                         0.9485      0.391      2.429      0.015       0.182       1.715\n",
       "'alternative rock'                  0.1924      0.197      0.974      0.330      -0.195       0.580\n",
       "'austindie'                         0.6574      0.505      1.301      0.194      -0.334       1.649\n",
       "'blues-rock'                       -0.3016      0.243     -1.241      0.215      -0.778       0.175\n",
       "'canadian metal'                    0.7083      0.857      0.827      0.409      -0.972       2.389\n",
       "'chillhop'                         -0.6586      0.847     -0.778      0.437      -2.320       1.003\n",
       "'columbus ohio indie'              -0.8179      0.535     -1.528      0.127      -1.868       0.233\n",
       "'dance-punk'                       -0.3826      0.571     -0.670      0.503      -1.503       0.738\n",
       "'deep new americana'                0.0373      0.218      0.171      0.864      -0.391       0.465\n",
       "'edm'                               0.6196      0.301      2.062      0.039       0.030       1.209\n",
       "'g funk'                           -0.5112      0.420     -1.217      0.224      -1.335       0.313\n",
       "'indie punk'                        0.0738      0.430      0.172      0.864      -0.769       0.917\n",
       "'pop'                               0.5996      0.276      2.175      0.030       0.059       1.140\n",
       "'progressive post-hardcore'         0.7197      0.449      1.603      0.109      -0.161       1.600\n",
       "'speed garage'                     -0.3341      0.646     -0.517      0.605      -1.602       0.933\n",
       "Radiohead                          -0.8771      0.821     -1.068      0.286      -2.489       0.735\n",
       "Str_Best                           -1.2647      0.330     -3.837      0.000      -1.911      -0.618\n",
       "Str_Acoustic                        3.1248      0.617      5.064      0.000       1.914       4.335\n",
       "Str_2000s                          -3.0511      0.381     -8.006      0.000      -3.799      -2.303\n",
       "dance_liveness_std                 -0.1096      0.030     -3.650      0.000      -0.169      -0.051\n",
       "==============================================================================\n",
       "Omnibus:                       31.737   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               33.675\n",
       "Skew:                          -0.395   Prob(JB):                     4.87e-08\n",
       "Kurtosis:                       2.868   Cond. No.                     6.39e+08\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.39e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With only the significant predictors in part(a),R2 score for test set: {}\".format(r2_score(y_train, results5.predict(X7))))\n",
    "print(\"With only the significant predictors in part(a),R2 score for test set: {}\".format(r2_test_e))\n",
    "results5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) Bootstrapping for 10% Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous parts, we observe that a smaller subset of original predictors may do a lot better in terms of test set prediction. Therefore, in part (f), we randomly choose 10% of predictors and fit a regression model. We do 500 iterations and record corresponding r2 test core and the associated predictors.\n",
    "\n",
    "We achieve a r2 test score of 0.21. However,since we are just randomly choosing predictors, this result could come from chance alone and may not be very robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "##bootstrapping for10% predictors\n",
    "r2_test=[]\n",
    "pred=[]\n",
    "for i in range(500):\n",
    "    train_col=[]\n",
    "    while len(train_col)==0:\n",
    "        for ele in x_train.columns:\n",
    "            u=np.random.uniform(0,1)\n",
    "            if u>0.9:\n",
    "                if ele!='Followers':\n",
    "                    train_col.append(ele)\n",
    "    pred.append(train_col)\n",
    "    x_train1 = x_train[train_col]\n",
    "    x_test1 = x_test[train_col]\n",
    "    multi2 =LinearRegression(fit_intercept=True)# no need to add constant when doing it this way\n",
    "    multi2.fit(x_train1, y_train)\n",
    "    r2_test.append(multi2.score(x_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "def findLargest(r2):\n",
    "    largest=r2[0]\n",
    "    count=0\n",
    "    for i in range(len(r2)):\n",
    "        if r2[i]>largest:\n",
    "            largest=r2[i]\n",
    "            count=i\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After bootstrapping for 10% of predictors, the best R2 score for test set: 0.21855216986291626\n",
      "\n",
      "\n",
      "The assocaited predictors are:['acousticness_mean', 'liveness_std', 'loudness_mean', 'valence_mean', 'followers_std', 'top_0_10', \" 'ambient'\", \" 'bebop'\", \" 'bluegrass'\", \" 'bow pop'\", \" 'chillhop'\", \" 'classic rock'\", \" 'classical piano'\", \" 'contemporary jazz'\", \" 'country dawn'\", \" 'deep talent show'\", \" 'desi hip hop'\", \" 'ethereal wave'\", \" 'freestyle'\", \" 'future garage'\", \" 'garage punk blues'\", \" 'heavy alternative'\", \" 'indie dream pop'\", \" 'indie garage rock'\", \" 'indie psych-rock'\", \" 'intelligent dance music'\", \" 'latin metal'\", \" 'metropopolis'\", \" 'modern uplift'\", \" 'motown'\", \" 'nashville sound'\", \" 'neo-industrial rock'\", \" 'new americana'\", \" 'noise rock'\", \" 'nu gaze'\", \" 'ok indie'\", \" 'piano blues'\", \" 'pop christmas'\", \" 'pop'\", \" 'power metal'\", \" 'progressive house'\", \" 'progressive post-hardcore'\", \" 'retro electro'\", \" 'scorecore'\", \" 'sludge metal'\", \" 'southern soul'\", \" 'swedish indie rock'\", \" 'teen pop'\", \" 'trance'\", \" 'triangle indie'\", \" 'tropical house'\", \" 'uk post-punk'\", \" 'vocal house'\", '\"children\\'s christmas\"', \"'alternative rock'\", \"'australian alternative rock'\", \"'baroque'\", \"'blackgaze'\", \"'canadian country'\", \"'canadian pop'\", \"'cantautor'\", \"'chillhop'\", \"'deep contemporary country'\", \"'desert blues'\", \"'djent'\", \"'dreamo'\", \"'drone'\", \"'east coast hip hop'\", \"'house'\", \"'indie pop'\", \"'indie poptimism'\", \"'indie psych-rock'\", \"'mellow gold'\", \"'minimal techno'\", \"'ninja'\", \"'skate punk'\", \"'vancouver indie'\", \"'wrestling'\", 'Rihanna', 'Chance The Rapper', 'Otis Redding', 'SZA', 'Birdy', 'Commodores', 'Str_Party', 'Str_Acoustic', 'Str_2000s', 'acoustic_key_std']\n"
     ]
    }
   ],
   "source": [
    "print(\"After bootstrapping for 10% of predictors, the best R2 score for test set: {}\".format(r2_test[findLargest(r2_test)]))\n",
    "print(\"\\n\")\n",
    "print('The assocaited predictors are:{}'.format(pred[findLargest(r2_test)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(g) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principle Component Analysis) is another way to reduce the number of predictors. Each component is a linear combination of all 949 orginal predictors. The components are ordered in such a wa so that the amount of captured observed variance descends. In part(g), we implement PCA here. We try different numbers of PCA components from 1 to 100 and choose the optimal number of PCA components according to test r2 score.\n",
    "\n",
    "We acheive the best r2 test score of 0.13 with 30 PCA components. Although we gain a higher test r2 score and also have less predictors, we lose a lot interpretability. We cannot pinpoint how change in one predictor will change the response varibable because each component is a linear combination of all original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13027619329603601"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "r2_test_pca=[]\n",
    "for i in range(1,100):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(x_train)\n",
    "    x_train_pca = pca.transform(x_train)\n",
    "    x_test_pca = pca.transform(x_test)\n",
    "    pca_regression_model = LinearRegression(fit_intercept=True)\n",
    "    pca_regression_model.fit(x_train_pca, y_train)\n",
    "    r2_test_pca.append(pca_regression_model.score(x_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA, the best R2 score for test set: 0.130276193296036\n",
      "The optimal number of components is: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"After PCA, the best R2 score for test set: {}\".format(r2_test_pca[findLargest(r2_test_pca)]))\n",
    "print(\"The optimal number of components is: {}\".format(findLargest(r2_test_pca)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(h) Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso and Ridge regularizations are also methods to penalize overly complex models. To penalize coefficeints that has large magnitude, Lasso and ridge include the magnitude of the cofficients in the loss functions. Specific, Lasso includes the sum of the absolute values of coefficients multiplied by a constant lambda. Ridge includes the sum of the square of coefficients multiplied by a constant lambda.In part (h), we fit Ridge and Lasso with cross validation and with lamda ranging from 1e^-5 to 10^5.\n",
    "\n",
    "With Lasso, the test r2 score is 0.109. With Ridge, the test r2 score is 0.112."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With lasso, R2 score for test set: 0.1094129599174094\n"
     ]
    }
   ],
   "source": [
    "#lasso CV\n",
    "lambda_list=[pow(10,i) for i in range(-5,5)]\n",
    "lasso_regression = LassoCV(alphas=lambda_list, fit_intercept=True)\n",
    "lasso_regression.fit(x_train, y_train)\n",
    "lasso_regression.get_params\n",
    "print(\"With lasso, R2 score for test set: {}\".format(lasso_regression.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ridge, R2 score for test set: 0.11234514421712671\n"
     ]
    }
   ],
   "source": [
    "#ridge\n",
    "ridge_regression = RidgeCV(cv=10,alphas=lambda_list, fit_intercept=True, normalize=True)\n",
    "ridge_regression.fit(x_train, y_train)\n",
    "print(\"With ridge, R2 score for test set: {}\".format(ridge_regression.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With lasso, R2 score for test set: 0.1094129599174094\n",
      "With ridge, R2 score for test set: 0.11234514421712671\n"
     ]
    }
   ],
   "source": [
    "print(\"With lasso, R2 score for test set: {}\".format(lasso_regression.score(x_test,y_test)))\n",
    "print(\"With ridge, R2 score for test set: {}\".format(ridge_regression.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 949 predictors in our original data, we encounter the problem of overfitting when constructing regression models. Therefore, for basline models, we focus on selecting a subgroup of predictors that perform well in terms of r2 test score. By using only the significant predictors from the full model, test r2 score improves a lot (from -2.7 to 0.8). PCA gives us r2 score of 0.13. Lasso gives up 0.109 and Ridge gives us 0.11. In addition, we should note that iteratively choosing 10% of predictors gives us the best baseline r2 metric (0.21). However, this model is not robust and tend toward overfitting since we do not methodologically choose the predictors. Still, we could use 0.21 as a reference when evaluating our future models. In summary, the most robust and predictive baseline model is PCA with r2 socre of 0.13. But we may use 0.21 as baseline r2_test metric for the furture. In addition, we should consider including top 30 artists predictors since they alone gives us 0.03 r2 test metric. On the other hand, we should carefully select which genre predictors to include since there are 865 of them and would lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
