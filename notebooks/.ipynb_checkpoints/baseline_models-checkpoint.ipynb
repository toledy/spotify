{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LassoCV,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import math\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.__version__\n",
    "plotly.tools.set_credentials_file(username='tingnoble', api_key='4zOe1ds7duZr15VxaRBp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "# Read-in the full data set\n",
    "data = pd.read_csv('data/Final_Dataframe.csv')\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "## train, test split##\n",
    "\n",
    "# A train/test split is constructed where 90% of the subsample is \n",
    "# the train data set and 10% the test data set.\n",
    "\n",
    "# Set train and test sizes\n",
    "train_size = 0.9\n",
    "test_size = 1-train_size\n",
    "\n",
    "# Function to return random train and test sets\n",
    "def data_splitter(df, train, validate=False, seed=9001):\n",
    "    \n",
    "    if validate:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        validate_end = int(validate * m) + train_end\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        validate = df.ix[perm[train_end:validate_end]]\n",
    "        test = df.ix[perm[validate_end:]]\n",
    "        return train, validate, test\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(df.index)\n",
    "        m = len(df)\n",
    "        train_end = int(train * m)\n",
    "        train = df.ix[perm[:train_end]]\n",
    "        test = df.ix[perm[train_end:]]\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "# Create train and test dataframes from subsample\n",
    "train_df, test_df = data_splitter(data, train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "# Median imputation of missing values\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=1)\n",
    "train_df = pd.DataFrame(imp.fit_transform(train_df), columns=data.columns)\n",
    "test_df = pd.DataFrame(imp.transform(test_df), columns=data.columns)\n",
    "\n",
    "train_df = train_df[train_df['Followers'] != 0]\n",
    "test_df = test_df[test_df['Followers'] != 0]\n",
    "\n",
    "# Final step: create y_train/x_train and y_test/x_test dataframes\n",
    "\n",
    "# Initialize the training data\n",
    "y_train = np.log(train_df['Followers'])\n",
    "x_train = train_df.drop('Followers', axis=1)\n",
    "\n",
    "# Initialize the testing data\n",
    "y_test = np.log(test_df['Followers'])\n",
    "x_test = test_df.drop('Followers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, our goal is to examine the baseline performance of simple regression models on the test set. We would use these baseline test set $R^2$ score as a reference for building more complex models. The models included in this section are mostly multi-linear regression models with different subset of predictors and possible polynomial/interaction terms. PCA and Lasso/Ridge are explored here as well. \n",
    "\n",
    "We choose to start with linear regression because of its high interpretability and low computational compexity. We can easily interpret the results of regression coeffcients by holding all other predictors constant and observing how a one unit increase in the specified predictor leads to a change in units indicated by the coefficent in the response variables. Linear regression also has a closed form solution for coefficients which reduces computational complexity and promotes efficiency.\n",
    "\n",
    "Linear regression has the following assumptions: \n",
    "* There is a linear relationship between response variables and predictors\n",
    "* Residuals are independent\n",
    "* Residuals are normally distributed\n",
    "* Residuals has constant variance \n",
    "\n",
    "To evaluate baseline models, we use the metric $R^2$, which is the proportion of overall variability of $Y$ explained by the model. We do note that $R^2$ has the tendency to overfit as it will always go up for the training set when we include more predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) Multi-Linear Regression Model with All Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we fit a multi-linear regression with all predictors that we have. This model would not neccessarily perform well because of the sheer number of predictors (949 in total). However, we want to conduct a sanity check through this model to make sure that prediction power is reasonable.\n",
    "\n",
    "The test $R^2$ score for multi-linear regression model with all predictors is -2.7797. Our prediction does a worse predcition job than the mean of response varaibles. This is evidence suggesting that we are overfitting our model with too many predictors. Therefore, going forward, we would like to fit a regression model with a subset of predictors in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "X=sm.add_constant(x_train)\n",
    "X_test=sm.add_constant(x_test)\n",
    "model=sm.OLS(y_train,X)\n",
    "results=model.fit()\n",
    "r2_test_a=r2_score(y_test,results.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with All Terms:\n",
      "R2 Score (Training) = 0.7656\n",
      "R2 Score (Test) = -2.7797\n"
     ]
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with All Terms:\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train,results.predict(X))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "figure1 = pd.DataFrame({\"p value\": results.pvalues, \"coef\": results.params})\n",
    "figure1 = figure1.drop(figure1.index[[0]])\n",
    "figure1 = figure1[figure1['p value'] < 0.05]\n",
    "figure1_10 = figure1.sort_values(by='p value', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "color_bars = [\"#000000\", \"#062d14\",\"#0c5a27\",\"#12873b\",\"#18b44f\",\"#1ed760\",\"4be782\",\n",
    "              \"#78eda1\",\"#a5f3c0\",\"#d2f9e0\",\"#ffffff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~tingnoble/7.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Bar(\n",
    "    x=figure1_10.index,\n",
    "    y=figure1_10['coef'],\n",
    "    marker=dict(\n",
    "        color=color_bars,\n",
    "        line=dict(\n",
    "            color='#ffffff',\n",
    "            width=1.5,\n",
    "        )\n",
    "    ),\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title='Multi-Linear Regression (All Predictors) Top 10 Significant Predictors',\n",
    "    xaxis=dict(\n",
    "        title='Predictor',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Coefficient',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Multi-Linear Regression (All Predictors) Top 10 Significant Predictors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part (b) Multi-Linear Regression Model with Top Artists Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our prelimnary EDA anlaysis, we believe that top artists would be a good predictor for the success of a playlist. Therefore, here, we fit two models in part (b) and (c) that only include predictors related artists. In part (b), we use the dummy variables of top 30 artists as predictors. As a note, for part (b), top artist are those who appear most often in playlists with 350,000+ followers. With more than 350,000 followers, a playlist will beat 80% of playlists in terms of followers. \n",
    "\n",
    "Our regression generates a test $R^2$ score of 0.0174, which is much better than -2.7797 in part (a). Therefore, there is good reason to consider these predictors in future model estimation.\n",
    "\n",
    "From the regression summary table, we obtain the list of significant top 30 artist predictors:\n",
    "* 'Galantis', 'Post Malone', 'Yo Gotti', 'Ellie Goulding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "top_30_artist_col=['Lil Wayne', 'Van Morrison', 'Galantis',\n",
    "       'Wiz Khalifa', 'Rihanna', 'Post Malone', 'Axwell /\\ Ingrosso',\n",
    "       'Young Thug', 'JAY Z', 'A$AP Rocky', 'Yo Gotti', 'Chance The Rapper',\n",
    "       'Led Zeppelin', 'Otis Redding', '21 Savage', 'Deorro', 'Elton John',\n",
    "       'SZA', 'Ty Dolla $ign', 'Ryan Adams', 'Birdy', 'Miguel', 'Niall Horan',\n",
    "       'Ellie Goulding', 'Commodores', 'Radiohead', 'SYML', 'First Aid Kit',\n",
    "       'Lord Huron']\n",
    "\n",
    "x_train_art=x_train[top_30_artist_col]\n",
    "x_test_art=x_test[top_30_artist_col]\n",
    "\n",
    "X1=sm.add_constant(x_train_art)\n",
    "X2=sm.add_constant(x_test_art)\n",
    "model2=sm.OLS(y_train,X1)\n",
    "results2=model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with Top 30 Artist Predictors:\n",
      "R2 Score (Training) = 0.0617\n",
      "R2 Score (Test) = 0.0174\n"
     ]
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with Top 30 Artist Predictors:\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train,results2.predict(X1))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_score(y_test,results2.predict(X2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "figure2 = pd.DataFrame({\"p value\": results2.pvalues, \"coef\": results2.params})\n",
    "figure2 = figure2.drop(figure2.index[[0]])\n",
    "figure2 = figure2[figure2['p value'] < 0.05]\n",
    "figure2_10 = figure2.sort_values(by='p value', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~tingnoble/9.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace0 = go.Bar(\n",
    "    x=figure2_10.index,\n",
    "    y=figure2_10['coef'],\n",
    "    marker=dict(\n",
    "        color=color_bars,\n",
    "        line=dict(\n",
    "            color='#ffffff',\n",
    "            width=1.5,\n",
    "        )\n",
    "    ),\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "layout = go.Layout(\n",
    "    title='Multi-Linear Regression (Top 30 Artist Predictors) Significant Predictors',\n",
    "    xaxis=dict(\n",
    "        title='Predictor',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Coefficient',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Multi-Linear Regression (Top 30 Artist Predictors) Significant Predictors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part (c) Multi-Linear Regression Model with Top Artists Counts Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to evaluate artist predictors in part (c). Here, top artists are defined differently from part (b).We first sum up the total number of followers for playlists that include an artist. Then, we rank the artists basing on the aggregated playlist followers. For part (c), the predictors are the number of top 10/10-20/20-30/30-40/40-50 artists that a playlist has.\n",
    "\n",
    "From the regression result, we observe that $R^2$ training result is 0.0126 and the test result is -0.0312. The significant predictors includes only \"top_30_40\". It seems that part (c) $R^2$ test score are lower than that in part (b), indicating that part (b) artist predictors has more power in predicting playlist followers than predictors in part (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "top_artist_count_columns = [\"top_0_10\", \"top_10_20\", \"top_20_30\", \"top_30_40\", \"top_40_50\"]\n",
    "x_train_art_count=x_train[top_artist_count_columns]\n",
    "x_test_art_count=x_test[top_artist_count_columns]\n",
    "\n",
    "X3=sm.add_constant(x_train_art_count)\n",
    "X4=sm.add_constant(x_test_art_count)\n",
    "model3=sm.OLS(y_train,X3)\n",
    "results3=model3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with Top Artist Count Predictors:\n",
      "R2 Score (Training) = 0.0126\n",
      "R2 Score (Test) = -0.0312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Followers</td>    <th>  R-squared:         </th> <td>   0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 07 Dec 2017</td> <th>  Prob (F-statistic):</th>  <td>0.00702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:37:36</td>     <th>  Log-Likelihood:    </th> <td> -3166.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1257</td>      <th>  AIC:               </th> <td>   6345.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1251</td>      <th>  BIC:               </th> <td>   6376.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    9.6438</td> <td>    0.097</td> <td>   99.900</td> <td> 0.000</td> <td>    9.454</td> <td>    9.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_0_10</th>  <td>    0.1075</td> <td>    0.274</td> <td>    0.392</td> <td> 0.695</td> <td>   -0.431</td> <td>    0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_10_20</th> <td>    0.2819</td> <td>    0.358</td> <td>    0.788</td> <td> 0.431</td> <td>   -0.420</td> <td>    0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_20_30</th> <td>   -0.1965</td> <td>    0.280</td> <td>   -0.702</td> <td> 0.483</td> <td>   -0.745</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_30_40</th> <td>    0.7624</td> <td>    0.273</td> <td>    2.789</td> <td> 0.005</td> <td>    0.226</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>top_40_50</th> <td>    0.7486</td> <td>    0.321</td> <td>    2.330</td> <td> 0.020</td> <td>    0.118</td> <td>    1.379</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>79.718</td> <th>  Durbin-Watson:     </th> <td>   1.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  83.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.595</td> <th>  Prob(JB):          </th> <td>6.73e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.574</td> <th>  Cond. No.          </th> <td>    4.32</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Followers   R-squared:                       0.013\n",
       "Model:                            OLS   Adj. R-squared:                  0.009\n",
       "Method:                 Least Squares   F-statistic:                     3.204\n",
       "Date:                Thu, 07 Dec 2017   Prob (F-statistic):            0.00702\n",
       "Time:                        14:37:36   Log-Likelihood:                -3166.5\n",
       "No. Observations:                1257   AIC:                             6345.\n",
       "Df Residuals:                    1251   BIC:                             6376.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          9.6438      0.097     99.900      0.000       9.454       9.833\n",
       "top_0_10       0.1075      0.274      0.392      0.695      -0.431       0.646\n",
       "top_10_20      0.2819      0.358      0.788      0.431      -0.420       0.984\n",
       "top_20_30     -0.1965      0.280     -0.702      0.483      -0.745       0.352\n",
       "top_30_40      0.7624      0.273      2.789      0.005       0.226       1.299\n",
       "top_40_50      0.7486      0.321      2.330      0.020       0.118       1.379\n",
       "==============================================================================\n",
       "Omnibus:                       79.718   Durbin-Watson:                   1.945\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               83.686\n",
       "Skew:                          -0.595   Prob(JB):                     6.73e-19\n",
       "Kurtosis:                       2.574   Cond. No.                         4.32\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with Top Artist Count Predictors:\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train,results3.predict(X3))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_score(y_test,results3.predict(X4))))\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part (d) Multi-Linear Regression Model with Genre Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our prelimnary EDA anlaysis, we also believe that genres would be a good predictor for playlist followers. Therefore, we fit a regression model with only genre predictors. Hereï¼Œeach predictors is a categorical variable indicating whether the playlist belongs to a specified genre.\n",
    "\n",
    "When we use all of the genre predictors, we see that training $R^2$ score is 0.5975 and test $R^2$ score is -3.7847. This again is the result of overfitting since we have 865 genre columns. Therefore, we fit another regression model with only significant genre predictors from the full genre regression model. This time, we have a training $R^2$ score of 0.0923 and test $R^2$ score of 0.0071. The number of significant genre predictors is 54.\n",
    "\n",
    "Therefore, a subset of genre predictors could still be important and should be considered for building future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "genre_columns = []\n",
    "\n",
    "for i in data.columns:\n",
    "    if (i[0].isalpha()==False):\n",
    "        genre_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "x_train_genre=x_train[genre_columns]\n",
    "x_test_genre=x_test[genre_columns]\n",
    "\n",
    "X5=sm.add_constant(x_train_genre)\n",
    "X6=sm.add_constant(x_test_genre)\n",
    "model4=sm.OLS(y_train,X5)\n",
    "results4=model4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with All Genre Predictors:\n",
      "R2 Score (Training) = 0.5975\n",
      "R2 Score (Test) = -3.7847\n",
      "Number of Genre Predictors = 865\n"
     ]
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with All Genre Predictors:\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train,results4.predict(X5))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_score(y_test,results4.predict(X6))))\n",
    "print(\"Number of Genre Predictors = {}\".format(len(genre_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "figure4 = pd.DataFrame({\"p value\": results4.pvalues, \"coef\": results4.params})\n",
    "figure4 = figure4.drop(figure4.index[[0]])\n",
    "figure4 = figure4[figure4['p value'] < 0.05]\n",
    "figure4_10 = figure4.sort_values(by='p value', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~tingnoble/11.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace0 = go.Bar(\n",
    "    x=figure4_10.index,\n",
    "    y=figure4_10['coef'],\n",
    "    marker=dict(\n",
    "        color=color_bars,\n",
    "        line=dict(\n",
    "            color='#ffffff',\n",
    "            width=1.5,\n",
    "        )\n",
    "    ),\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "layout = go.Layout(\n",
    "    title='Multi-Linear Regression (Genre Predictors) Top 10 Significant Predictors',\n",
    "    xaxis=dict(\n",
    "        title='Predictor',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Coefficient',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Multi-Linear Regression (Genre Predictors) Top 10 Significant Predictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "sig_genre=results4.pvalues[results4.pvalues < 0.05].index[1:55]\n",
    "x_train_genre=x_train[sig_genre]\n",
    "x_test_genre=x_test[sig_genre]\n",
    "\n",
    "X9=sm.add_constant(x_train_genre)\n",
    "X10=sm.add_constant(x_test_genre)\n",
    "model9=sm.OLS(y_train,X9)\n",
    "results9=model9.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with Significant Genre Predictors:\n",
      "R2 Score (Training) = 0.0923\n",
      "R2 Score (Test) = 0.0071\n",
      "Number of Genre Predictors = 54\n"
     ]
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with Significant Genre Predictors:\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train,results9.predict(X9))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_score(y_test,results9.predict(X10))))\n",
    "print(\"Number of Genre Predictors = {}\".format(len(sig_genre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Multi-Linear Regression with Signifcant Predictors from part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in part (a), our model includes all predictors and thus tend towards overfitting. In part (e), we fit a model with siginificant predictors from model in part (a) to reduce overfitting. We have a total of 49 predictors (cut down from 949 originally).\n",
    "\n",
    "Test $R^2$ score goes up to 0.0826, which is the best $R^2$ score we have achieved so far. This indicates that our previous model indeed suffers from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "#fit a multilinear regression model with significant predictors\n",
    "sig_preds=results.pvalues[results.pvalues < 0.05].index\n",
    "x_train2 = x_train[sig_preds]\n",
    "x_test2 = x_test[sig_preds]\n",
    "\n",
    "X7=sm.add_constant(x_train2)\n",
    "X8=sm.add_constant(x_test2)\n",
    "model5=sm.OLS(y_train,X7)\n",
    "results5=model5.fit()\n",
    "r2_test_e=r2_score(y_test,results5.predict(X8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Multi-Linear Regression with Significant Predictors from part (a):\n",
      "R2 Score (Training) = 0.2229\n",
      "R2 Score (Test) = 0.0826\n"
     ]
    }
   ],
   "source": [
    "print(\"For Multi-Linear Regression with Significant Predictors from part (a):\")\n",
    "print(\"R2 Score (Training) = {:.4f}\".format(r2_score(y_train, results5.predict(X7))))\n",
    "print(\"R2 Score (Test) = {:.4f}\".format(r2_test_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) Bootstrapping for 10% Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous parts, we observe that a smaller subset of original predictors may perform better in terms of test set prediction. Therefore, in part (f), we randomly choose 10% of predictors and fit a regression model. We do 500 iterations and record the corresponding $R^2$ test score and the associated predictors.\n",
    "\n",
    "We achieve a $R^2$ test score of 0.2246. However,since predictors are being randomly chosen, this result could come from chance alone and may not be very robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "##bootstrapping for10% predictors\n",
    "r2_test=[]\n",
    "pred=[]\n",
    "for i in range(500):\n",
    "    train_col=[]\n",
    "    while len(train_col)==0:\n",
    "        for ele in x_train.columns:\n",
    "            u=np.random.uniform(0,1)\n",
    "            if u>0.9:\n",
    "                if ele!='Followers':\n",
    "                    train_col.append(ele)\n",
    "    pred.append(train_col)\n",
    "    x_train1 = x_train[train_col]\n",
    "    x_test1 = x_test[train_col]\n",
    "    multi2 =LinearRegression(fit_intercept=True)# no need to add constant when doing it this way\n",
    "    multi2.fit(x_train1, y_train)\n",
    "    r2_test.append(multi2.score(x_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "def findLargest(r2):\n",
    "    largest=r2[0]\n",
    "    count=0\n",
    "    for i in range(len(r2)):\n",
    "        if r2[i]>largest:\n",
    "            largest=r2[i]\n",
    "            count=i\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Bootstrapping for 10% of predictors, Best R2 Score (Test) = 0.2246\n",
      "The Associated Predictors are :['acousticness_std', 'dance_mean', 'key_std', 'mode_mean', 'valence_mean', 'popularity_mean', 'popularity_std', 'top_30_40', \" 'alternative roots rock'\", \" 'art rock'\", \" 'austindie'\", \" 'boston rock'\", \" 'canadian pop'\", \" 'catstep'\", \" 'ccm'\", \" 'celtic rock'\", \" 'chillhop'\", \" 'classic funk rock'\", \" 'cool jazz'\", \" 'deep talent show'\", \" 'deep tech house'\", \" 'detroit techno'\", \" 'escape room'\", \" 'filmi'\", \" 'filter house'\", \" 'garage rock'\", \" 'german indie'\", \" 'glitch'\", \" 'indie poptimism'\", \" 'jazz'\", \" 'jump blues'\", \" 'memphis blues'\", \" 'neo soul'\", \" 'new romantic'\", \" 'noise punk'\", \" 'ok indie'\", \" 'power pop'\", \" 'progressive house'\", \" 'rockabilly'\", \" 'soundtrack'\", \" 'symphonic rock'\", \" 'teen pop'\", \" 'thrash core'\", \" 'urban contemporary'\", \" 'vegan straight edge'\", \" 'worship'\", \"'alternative americana'\", \"'art rock'\", \"'aussietronica'\", \"'baroque'\", \"'bay area indie'\", \"'bluegrass'\", \"'brass band'\", \"'britpop'\", \"'bubblegum dance'\", \"'chamber pop'\", \"'compositional ambient'\", \"'contemporary jazz'\", \"'dance pop'\", \"'deep big room'\", \"'deep contemporary country'\", \"'deep euro house'\", \"'deep swedish indie pop'\", \"'djent'\", \"'freak folk'\", \"'halloween'\", \"'indie garage rock'\", \"'indie pop'\", \"'indie poptimism'\", \"'japanese city pop'\", \"'lift kit'\", \"'lowercase'\", \"'melodic metalcore'\", \"'post-teen pop'\", \"'uk drill'\", \"'underground hip hop'\", \"'vancouver indie'\", \"'vapor soul'\", 'Yo Gotti', 'Chance The Rapper', 'Str_2000s', 'Str_1980s', 'Str_1960s']\n"
     ]
    }
   ],
   "source": [
    "print(\"After Bootstrapping for 10% of predictors, Best R2 Score (Test) = {:.4f}\".format(r2_test[findLargest(r2_test)]))\n",
    "print('The Associated Predictors are :{}'.format(pred[findLargest(r2_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g) Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) is another way to reduce the number of predictors. Each component is a linear combination of all 949 orginal predictors. The components are ordered in such a way so that the amount of captured observed variance descreases. In part (g), linear regression is performed on the principal components of the PCA transformation. We attempt different numbers of PCA components from 1 to 100 and choose the optimal number of PCA components according to test $R^2$ score. \n",
    "\n",
    "We achieve the best $R^2$ test score of 0.1307 with 30 PCA components. Although we gain a higher test $R^2$ score and also have less predictors, we lose interpretability. We cannot pinpoint how a change in one predictor will change the response varibable because each component is a linear combination of all original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "hide": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "r2_test_pca=[]\n",
    "for i in range(1,100):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(x_train)\n",
    "    x_train_pca = pca.transform(x_train)\n",
    "    x_test_pca = pca.transform(x_test)\n",
    "    pca_regression_model = LinearRegression(fit_intercept=True)\n",
    "    pca_regression_model.fit(x_train_pca, y_train)\n",
    "    r2_test_pca.append(pca_regression_model.score(x_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA, Best R2 Score (Test) = 0.1307\n",
      "The Optimal Number of Components = 30\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAHyCAYAAABWEZKLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQrYCCGEEELU3P8BeRdTjV09t2UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b6de9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"After PCA, Best R2 Score (Test) = {:.4f}\".format(r2_test_pca[findLargest(r2_test_pca)]))\n",
    "print(\"The Optimal Number of Components = {}\".format(findLargest(r2_test_pca)+1))\n",
    "green_col = np.divide([30,215,96],255)\n",
    "n=[ele for ele in range(1,100)]\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 8))\n",
    "ax.scatter(n,r2_test_pca, color=green_col, alpha=0.8)\n",
    "ax.set_title(\"PCA test performace\")\n",
    "ax.set_xlabel(\"Number of PCA components\")\n",
    "ax.set_ylabel(\"R2 Score (Test)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h) Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso and Ridge regularizations are also methods to penalize overly complex models. To penalize coefficients that have large magnitudes, Lasso and ridge include the magnitude of the cofficients in the loss functions. Specifically, Lasso minimizes the following function:\n",
    "\n",
    "$$\\sum_{i=1}^n (Y_i-\\sum_{j=1}^p X_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "Ridge minimizes the following function:\n",
    "\n",
    "$$\\sum_{i=1}^n (Y_i-\\sum_{j=1}^p X_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p (\\beta_j)^2$$\n",
    "\n",
    "In part (h), we fit Ridge and Lasso with cross validation and with lamda ranging from 1e^-5 to 10^5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
       "    copy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000,\n",
       "    n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lasso\n",
    "lambda_list=[pow(10,i) for i in range(-5,5)]\n",
    "lasso_regression = LassoCV(alphas=lambda_list, fit_intercept=True)\n",
    "lasso_regression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
       "    cv=10, fit_intercept=True, gcv_mode=None, normalize=True, scoring=None,\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ridge\n",
    "ridge_regression = RidgeCV(cv=10,alphas=lambda_list, fit_intercept=True, normalize=True)\n",
    "ridge_regression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "hide": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Lasso Regularization, R2 Score (Test) = 0.1094\n",
      "With Ridge Regularization, R2 Score (Test) = 0.0959\n"
     ]
    }
   ],
   "source": [
    "print(\"With Lasso Regularization, R2 Score (Test) = {:.4f}\".format(lasso_regression.score(x_test, y_test)))\n",
    "print(\"With Ridge Regularization, R2 Score (Test) = {:.4f}\".format(ridge_regression.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 949 predictors in our original data, we encounter the problem of overfitting when constructing regression models. Therefore, for baseline models, we focus on selecting a subgroup of predictors that perform well in terms of $R^2$ test score. By using only the significant predictors from the full model, test $R^2$ score improves significantly (from -2.7 to 0.8). PCA gives us an $R^2$ score of 0.1307. Lasso gives up 0.1094 and Ridge gives us 0.0959. In addition, we should note that iteratively choosing 10% of predictors gives us the best baseline $R^2$ metric (0.2246). However, this model is not robust and tend toward overfitting since we do not methodologically choose the predictors. Still, we could use 0.2246 as a reference when evaluating our future models. In summary, the most robust and predictive baseline model is PCA with $R^2$ score of 0.1307. We may use 0.2246 as baseline $R^2$ test metric. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
